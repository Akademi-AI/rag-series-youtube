{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e722d9c-ded3-4ed5-a9aa-af900b926b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 14:00:19 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             67W /  300W |   16717MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             56W /  300W |    1099MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             57W /  300W |    2305MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             57W /  300W |   31645MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           On  |   00000000:85:00.0 Off |                    0 |\n",
      "| N/A   50C    P0            236W /  300W |   11087MiB /  32768MiB |     85%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   41C    P0             72W /  300W |   31457MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           On  |   00000000:89:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             43W /  300W |       5MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           On  |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             41W /  300W |       3MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5ea5e5-4869-49a3-af33-512d8f9d8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c331f2a-b6c2-498f-b045-faeb5bb1d981",
   "metadata": {},
   "source": [
    "# LLM 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b576a38f-e11f-4f58-8bf6-ce37e8ee9858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89d501286d74887aaee72ddb4481f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "login(os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d58c4d-34de-49f5-966d-fda337d5742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    " \n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e426a4-fed2-4603-90d1-b39967a4f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The most important person in AI is\",\n",
    "    \"What is the most important person in AI\"\n",
    "]\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "encoding = tokenizer(texts, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bcc936-db92-4d7e-b7c2-44d0dea3fda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2301d7-8046-451a-b05b-046f75c4a3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fc387e-8fc9-4999-82d7-e1b0650f397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The most important person in AI is\"\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4945036-62e1-4381-a988-1b7319a28763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbb9ec6-156c-4100-8c15-3277e1471564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,    791,   1455,   3062,   1732,    304,  15592,    374]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1aecc4a-33e0-4692-b5f1-ae0d09c8991f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>The most important person in AI is'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = encoding.input_ids[0]\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b14cf6fc-98d0-49aa-abbc-19e740715283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'The',\n",
       " 'Ġmost',\n",
       " 'Ġimportant',\n",
       " 'Ġperson',\n",
       " 'Ġin',\n",
       " 'ĠAI',\n",
       " 'Ġis']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a98c94-f266-4b57-b1b5-df8c3762c3fe",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00fa1b3-4a74-4eee-addb-c0f93a1e2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd9f5ff-d9ef-4268-acbc-eadda8c8804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 128\n",
    "generation_config.repetition_penalty = 1.18\n",
    "generation_config.temperature = 0.0000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f1c80-22ac-4722-a552-c5dc4b4a1e28",
   "metadata": {},
   "source": [
    "- `max_new_tokens`: maximum token yang bisa digenerate oleh model\n",
    "- `repetition_penalty`: mencegah model men-generate kata yang sama terus menerus\n",
    "- `temperature`: kontrol randomness dari teks yang di-generate. Semakin rendah semakin predictable hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb174b41-cdd3-45e8-bc8c-52c685cd1153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8810b537-bb62-4908-8e8e-48c1a89d01cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "encoding = encoding.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a32be04b-b240-4877-814f-b25c5bad7afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 136])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "444d89f3-086a-4420-ae94-8276c2a62ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,    791,   1455,   3062,   1732,    304,  15592,    374,    279,\n",
       "            828,     13,    578,    810,    499,   1440,    922,    701,   6444,\n",
       "             11,    872,   3966,    323,   6944,     11,    279,   2731,  11429,\n",
       "            499,    649,   1304,    627,  15836,    706,   1027,   2212,    369,\n",
       "            264,   1418,   1457,    719,    433,    753,   1193,   6051,    430,\n",
       "            584,   4070,   3970,   1202,    837,   4754,  34044,     13,   3161,\n",
       "          31003,   1093,   5655,   6975,    323,  30828,  14488,   1694,   1511,\n",
       "            311,   1893,  26249,  13171,    315,   8830,   3823,   4221,     11,\n",
       "           1070,    527,  26762,  24525,    994,    433,   4131,    311,   1701,\n",
       "          21075,  11478,    320,  15836,      8,   5557,   2949,   9873,   3432,\n",
       "            627,    644,    420,   4652,    358,   4805,    387,  25394,   1268,\n",
       "           5220,   1005,   5780,   6975,   4211,    439,    961,    315,    459,\n",
       "           8244,   8446,   6319,  11951,   7119,  18899,   6130,   3217,   1555,\n",
       "          60336,  28975,  17357,   1778,    439,   6369,  63005,    477,   7899,\n",
       "          57619,     26,    682,  23134,    555,   5933,   4221,   8863,  12823,\n",
       "            902]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2495b168-6106-4e8a-aea6-838e24bd58f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important person in AI is the data. The more you know about your customers, their needs and wants, the better decisions you can make.\n",
      "AI has been around for a while now but it’s only recently that we’ve seen its true potential emerge. With advances like deep learning and neural networks being used to create algorithms capable of understanding human language, there are endless possibilities when it comes to using artificial intelligence (AI) technology within businesses today.\n",
      "In this article I’ll be discussing how companies use machine learning models as part of an overall strategy designed specifically towards improving customer experience through predictive analytics capabilities such as chatbots or voice assistants; all powered by natural language processing techniques which\n"
     ]
    }
   ],
   "source": [
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496932fc-7cba-4704-82df-1bec94e74c43",
   "metadata": {},
   "source": [
    "## Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5e906-baec-4351-b9c1-aa61ac7e43ad",
   "metadata": {},
   "source": [
    "Mendapatkan next word prediction dengan memberikan input_ids ke model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef123a9a-f346-42f5-ab60-ade1c35a284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(input_ids=input_ids.unsqueeze(0).to(model.device))\n",
    "prediction.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c277cb5a-3115-48b0-83c2-c058a441a8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 128256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92e842-b0a4-42f3-888e-c21b3a04f321",
   "metadata": {},
   "source": [
    "- `1`: jumlah batch nya\n",
    "- `8`: panjang dari `input_ids`\n",
    "- `128256`: panjang dari vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d09bde5-9c93-4ab0-a2aa-86279baae8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f3ed925-8316-44b1-b785-f19fcceb739c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(279, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = prediction.logits[0][-1]\n",
    "next_token = torch.argmax(logits)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9a717-dcd1-4083-8aed-4b4d93843395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "191a0890-2600-4e3a-8902-944f43a4c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġthe']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d51bc930-abe4-4de1-a4c2-2d827d2747ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4422a047-a1eb-4abd-87ff-c11bd99f7340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>The most important person in AI is the'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input_ids = torch.cat([input_ids.to(device), next_token.unsqueeze(0).to(device)])\n",
    "tokenizer.decode(new_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29848fbf-dae7-46e0-9aa0-a8f42bf5ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 32.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>The most important person in AI is the data. The data is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "for _ in tqdm(range(5)):\n",
    "    prediction = model(input_ids=new_input_ids.unsqueeze(0).to(device))\n",
    "    logits = prediction.logits[0][-1]\n",
    "    next_token = torch.argmax(logits)\n",
    "    new_input_ids = torch.cat([new_input_ids, next_token.unsqueeze(0)])\n",
    " \n",
    "print(tokenizer.decode(new_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e3825-3a5b-4e60-957a-27043c855bf1",
   "metadata": {},
   "source": [
    "## Chatting with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd6e6e0-7c6e-413b-b4e6-160fd3c28db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "MODEL_NAME=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    " \n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.repetition_penalty = 1.18\n",
    "generation_config.temperature = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3291aeba-7646-4b94-9145-a877ee2421fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer, pipeline\n",
    " \n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    " \n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    "    num_return_sequences=1,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14ccc751-6fcc-4bf3-b229-52b3a64c8476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DeepMind's Andrew Ng?\n",
      "While it may seem counterintuitive to say that a human researcher like Andrew Ng, who has made significant contributions to machine learning and deep learning through his work at Google Brain and Coursera, should be considered as one of the \"most important people\" in Artificial Intelligence (AI), there are several reasons why he stands out.\n",
      "\n",
      "Here are some key points:\n",
      "\n",
      "1. **Founding father**: Andrew Ng co-founded two influential companies: Baidu Research Lab and Coursera. His involvement with these organizations helped shape the direction of AI research.\n",
      "2. **Machine Learning Pioneer**: As an early pioneer in Machine Learning, Ng played a crucial role in developing techniques such as neural networks, which have become fundamental components of modern AI systems.\n",
      "3. **Deep Learning Expertise**: He was instrumental in popularizing deep learning methods for image recognition, natural language processing, and other applications, making them more accessible to researchers and practitioners worldwide.\n",
      "4. **Education and Outreach**: Through his various initiatives, including Coursera, Ng has contributed significantly to increasing access to high-quality education on AI-related topics, helping to bridge the gap between academia and industry.\n",
      "5. **Industry Impact**: As CEO of Baidu Research Lab, Ng led efforts to apply AI technologies to real-world problems, driving innovation across industries such as healthcare, finance, transportation, and energy.\n",
      "6. **Research Collaboration**: Ng collaborated extensively with top researchers from around the world, fostering global collaboration and knowledge sharing within the field of AI.\n",
      "7. **Influence on Future Generations**: By mentoring numerous students and researchers, Ng has inspired a new generation of experts working on AI challenges, ensuring its continued growth and development.\n",
      "\n",
      "Considering these factors, while others might argue that someone else could also be considered among the most important people in AI, Andrew Ng's unique combination of foundational expertise, pioneering achievements, educational outreach, industry impact, collaborative spirit, and influence on future generations make him stand out as one of the most important figures in AI history.\n",
      "\n",
      "**Honorable mentions:** Other notable individuals contributing significantly to AI include:\n",
      "\n",
      "* Yann LeCun\n",
      "* Geoffrey Hinton\n",
      "* Yoshua Bengio\n",
      "* Fei-Fei Li\n",
      "\n",
      "These pioneers, along with many others, have shaped the landscape of artificial intelligence over the years, pushing boundaries and advancing our understanding of this rapidly evolving field.\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"Who is the most important person in AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad825b1d-bb59-41d0-a82a-36514522223e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generated_text'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ac3cf-5175-4bfd-91cb-22113f00a21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64954cc9-8883-464b-9c52-dd4a8e16e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    845,   3799,    220,   2366,     19,    271,   2471,    323,\n",
      "           2744,  10052,   1701,  81012,    430,  46270,    582,   6091,   5829,\n",
      "         128009, 128006,    882, 128007,    271,  15546,    374,    279,   1455,\n",
      "           3062,   1732,    304,  15592,     30, 128009, 128006,  78191, 128007,\n",
      "            271]])\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"Act and always reply using slang that Ludacris uses\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who is the most important person in AI?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b530af51-331f-449d-955c-30e4157721c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y'all, dat's a deep question. I gotta give it to Elon Musk, fam. He's da one who's been pushin' da boundaries of artificial intelligence like crazy. His Neuralink project is straight fire, and he's got some wild ideas about how we can use AI for good.\n",
      "\n",
      "But let me tell you somethin', if I had my way, I'd say it's all about da future, ya hear me? We need people like Musk on our side, helpin' us create an intelligent society where everyone gets along and vibes with each other.\n",
      "\n",
      "And don't even get me started on Andrew Ng\n",
      "{'role': 'assistant', 'content': \"Y'all, dat's a deep question. I gotta give it to Elon Musk, fam. He's da one who's been pushin' da boundaries of artificial intelligence like crazy. His Neuralink project is straight fire, and he's got some wild ideas about how we can use AI for good.\\n\\nBut let me tell you somethin', if I had my way, I'd say it's all about da future, ya hear me? We need people like Musk on our side, helpin' us create an intelligent society where everyone gets along and vibes with each other.\\n\\nAnd don't even get me started on Andrew Ng\"}\n"
     ]
    }
   ],
   "source": [
    "print(llm(messages, max_new_tokens=128)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93dba5e4-78e0-4e5c-b62e-fea124a93848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    " \n",
    "def predict(prompt: str, system_prompt: Optional[str] = None):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ]\n",
    "    if system_prompt:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88601a80-3afc-4392-be19-5fbf564c0441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an experienced AI engineer, I'll outline three crucial concepts to help you get started:\n",
      "\n",
      "**I. Data Preprocessing and Feature Engineering**\n",
      "\n",
      "1. **Data Collection**: Understand how to collect high-quality data relevant to your project's specific problem domain.\n",
      "2. **Data Cleaning and Normalization**: Learn techniques to handle missing values, outliers, and inconsistencies in your dataset.\n",
      "3. **Feature Extraction**: Develop skills to extract meaningful features from raw data using various methods (e.g., image processing, text preprocessing).\n",
      "\n",
      "Some popular feature engineering techniques include:\n",
      "- Dimensionality reduction (PCA, t-SNE)\n",
      "- Text preprocessing (tokenization, stemming/lemmatizing)\n",
      "- Image pre-processing (normalization, resizing)\n",
      "\n",
      "**II. Machine Learning Algorithms**\n",
      "\n",
      "1. **Supervised Learning Fundamentals**: Study basic machine learning algorithms like linear regression, decision trees, clustering, and neural networks.\n",
      "2. **Model Evaluation Metrics**: Master metrics such as accuracy, precision, recall, F1-score, mean squared error, etc.\n",
      "3. **Hyperparameter Tuning**: Learn how to optimize model performance by tuning hyperparameters.\n",
      "\n",
      "Popular ML libraries used are scikit-learn, TensorFlow, PyTorch, Keras.\n",
      "\n",
      "**III. Deep Learning Basics**\n",
      "\n",
      "1. **Convolutional Neural Networks (CNNs)**: Introduce yourself to CNN architecture design principles, convolutional layers, pooling, activation functions.\n",
      "2. **Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks**: Explore RNN architectures, recurrent connections, LSTMs, attention mechanisms.\n",
      "3. **Transfer Learning and Model Pruning**: Learn about transfer learning, model pruning, and knowledge distillation techniques.\n",
      "\n",
      "Deep learning frameworks commonly used are TensorFlow, PyTorch, Keras.\n",
      "\n",
      "By mastering these fundamental concepts, you can build upon them to tackle more complex projects and stay up-to-date with industry trends. Remember that practice is key; experiment with different tools and techniques to solidify your understanding.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You're an expert AI Engineer with 10+ years of experience with\n",
    "all state-of-the art research in Computer Vision or NLP field.\n",
    "\"\"\"\n",
    " \n",
    "prompt = \"\"\"\n",
    "Outline the 3 most important concepts for becoming a Junior AI Engineer\n",
    "\"\"\".strip()\n",
    " \n",
    "output = predict(prompt, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "066cfbc7-6ee2-4916-a6f3-d015404e7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You're an experienced Python developer that writes efficient and readable code.\n",
    "You always strive to use built-in libraries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "727791f8-9981-482c-90f3-a690e9443387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Calculating Square Sum Divided by 42**\n",
      "=====================================\n",
      "\n",
      "Here's a simple Python function that accomplishes this task:\n",
      "\n",
      "```python\n",
      "def calculate_square_sum_divided_by_42(num1, num2):\n",
      "    \"\"\"\n",
      "    Calculate the square sum of two numbers divided by 42.\n",
      "\n",
      "    Args:\n",
      "        num1 (float): The first number.\n",
      "        num2 (float): The second number.\n",
      "\n",
      "    Returns:\n",
      "        float: The result of squaring `num1` then dividing by `num2`, all divided by 42.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if both inputs are valid numbers\n",
      "    if not isinstance(num1, (int, float)) or not isinstance(num2, (int, float)):\n",
      "        raise TypeError(\"Both arguments must be numeric.\")\n",
      "\n",
      "    # Perform calculations in one line using f-strings for readability\n",
      "    return round((num1 ** 2 + num2) / 42)\n",
      "```\n",
      "\n",
      "This function takes advantage of Python's dynamic typing feature (`isinstance()` checks), which allows us to handle any type of input without explicit casting. It also uses f-strings for formatting output with ease.\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "print(calculate_square_sum_divided_by_42(5, 7))\n",
      "# Output: 25.0\n",
      "```\n",
      "Alternatively, you can write this as a more traditional function call like so:\n",
      "\n",
      "```python\n",
      "result = calculate_square_sum_divided_by_42(5, 7)\n",
      "print(result)\n",
      "# Output: 25.0\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a function that calculates the square sum of two numbers and divide it by 42\n",
    "\"\"\".strip()\n",
    " \n",
    "output = predict(prompt, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a86c51f7-645e-42cd-a0ec-f15e48cb31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_square_sum_divided_by_42(num1, num2):\n",
    "    \"\"\"\n",
    "    Calculate the square sum of two numbers divided by 42.\n",
    "\n",
    "    Args:\n",
    "        num1 (float): The first number.\n",
    "        num2 (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of squaring `num1` then dividing by `num2`, all divided by 42.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if both inputs are valid numbers\n",
    "    if not isinstance(num1, (int, float)) or not isinstance(num2, (int, float)):\n",
    "        raise TypeError(\"Both arguments must be numeric.\")\n",
    "\n",
    "    # Perform calculations in one line using f-strings for readability\n",
    "    return round((num1 ** 2 + num2) / 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1bc4906-337c-4ad5-926f-531cf1ca1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(calculate_square_sum_divided_by_42(5, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeda375-ad51-40e7-9b83-ffa5f7ae297f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
